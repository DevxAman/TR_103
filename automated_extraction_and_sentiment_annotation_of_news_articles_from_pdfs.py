# -*- coding: utf-8 -*-
"""Automated Extraction and Sentiment Annotation of News Articles from PDFs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UeGI9CHKm8s5b0VODzIadI_KQT1hIbiu
"""

!pip install pdfplumber PyPDF2 pytesseract pdf2image transformers torch openpyxl pandas numpy
!apt-get update && apt-get install -y tesseract-ocr

"""
Production-Grade PDF News Sentiment Analysis Pipeline
Complete Clean Version for Google Colab
"""

import os
import re
import logging
from datetime import datetime
from typing import Dict, List, Optional
import pandas as pd
import numpy as np

# PDF Processing
import pdfplumber
from PyPDF2 import PdfReader

# OCR (optional)
try:
    import pytesseract
    from pdf2image import convert_from_path
    HAS_OCR = True
except:
    HAS_OCR = False

# NLP
import torch
from transformers import pipeline as hf_pipeline

# Excel
from openpyxl import Workbook
from openpyxl.styles import Alignment, PatternFill


# LOGGING

logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)


# PDF TEXT EXTRACTION


def extract_with_pdfplumber(pdf_path):
    """Extract text using pdfplumber"""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text.strip() if text else None
    except Exception as e:
        logger.warning(f"pdfplumber failed: {str(e)[:80]}")
        return None

def extract_with_pypdf2(pdf_path):
    """Extract text using PyPDF2"""
    try:
        with open(pdf_path, 'rb') as file:
            reader = PdfReader(file)
            text = ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text.strip() if text else None
    except Exception as e:
        logger.warning(f"PyPDF2 failed: {str(e)[:80]}")
        return None

def extract_with_ocr(pdf_path):
    """Extract text using OCR"""
    if not HAS_OCR:
        return None
    try:
        logger.info("  -> Attempting OCR...")
        images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=3)
        text = ""
        for image in images:
            ocr_text = pytesseract.image_to_string(image)
            if ocr_text:
                text += ocr_text + "\n"
        return text.strip() if text else None
    except Exception as e:
        logger.error(f"OCR failed: {str(e)[:80]}")
        return None

def extract_text_from_pdf(pdf_path):
    """Main extraction with fallback"""
    logger.info(f"Processing: {os.path.basename(pdf_path)}")

    # Try pdfplumber
    text = extract_with_pdfplumber(pdf_path)
    if text and len(text) > 100:
        logger.info(f"  -> Extracted via pdfplumber ({len(text)} chars)")
        return text

    # Try PyPDF2
    text = extract_with_pypdf2(pdf_path)
    if text and len(text) > 100:
        logger.info(f"  -> Extracted via PyPDF2 ({len(text)} chars)")
        return text

    # Try OCR
    text = extract_with_ocr(pdf_path)
    if text and len(text) > 100:
        logger.info(f"  -> Extracted via OCR ({len(text)} chars)")
        return text

    logger.error(f"  -> Failed to extract text")
    return None


# TEXT CLEANING


def clean_text(text):
    """Remove all noise from text - LIGHTER VERSION"""

    # Remove URLs only
    text = re.sub(r'https?://[^\s]+', '', text)
    text = re.sub(r'www\.[^\s]+', '', text)

    # Remove "Printed from" lines
    text = re.sub(r'Printed from.*?(?=\n|$)', '', text, flags=re.IGNORECASE)

    # DON'T remove dates/times yet - we need them for date extraction!

    # Collapse excessive whitespace
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)

    # Clean lines but keep content
    lines = []
    for line in text.split('\n'):
        line = line.strip()
        if len(line) > 0:
            lines.append(line)

    return '\n'.join(lines)


# METADATA EXTRACTION


def extract_headline(text):
    """Extract headline from text"""
    lines = text.split('\n')

    for i, line in enumerate(lines[:15]):
        line = line.strip()

        # Skip if too short
        if len(line) < 20:
            continue

        # Skip dates
        if re.match(r'^\d{1,2}[/-]\d{1,2}[/-]\d{2,4}', line):
            continue

        # Skip bylines
        if re.match(r'^(?:TNN|PTI|ANI|TIMESOFINDIA)', line, re.IGNORECASE):
            continue

        # Skip pipe separators
        if '|' in line and len(line) < 50:
            continue

        # Skip metadata
        if re.match(r'^(?:Printed|Also|Share|Comment|TIMESOFINDIA|NEW DELHI)', line, re.IGNORECASE):
            continue

        # Skip mostly uppercase
        if sum(1 for c in line if c.isupper()) > len(line) * 0.7:
            continue

        # This is the headline
        return line

    return "NO_HEADLINE"

def extract_date(text):
    """Extract publication date - AGGRESSIVE version"""

    # Pattern 1: Nov 26, 2025, 11.18 PM IST (most common in TOI)
    match = re.search(r'([A-Z][a-z]{2,8})\s+(\d{1,2}),?\s+(\d{4})', text, re.IGNORECASE)
    if match:
        try:
            month_str = match.group(1)
            day = match.group(2)
            year = match.group(3)
            date_str = f"{month_str} {day} {year}"
            date_obj = datetime.strptime(date_str, "%b %d %Y")
            return date_obj.strftime("%Y-%m-%d")
        except:
            try:
                date_obj = datetime.strptime(date_str, "%B %d %Y")
                return date_obj.strftime("%Y-%m-%d")
            except:
                pass

    # Pattern 2: 11/27/25 or 27-11-2025
    match = re.search(r'(\d{1,2})[/-](\d{1,2})[/-](\d{2,4})', text)
    if match:
        try:
            m, d, y = match.groups()
            if len(y) == 2:
                y = "20" + y
            date_obj = datetime(int(y), int(m), int(d))
            return date_obj.strftime("%Y-%m-%d")
        except:
            pass

    return None

def extract_article_content(text, headline):
    """Extract main article body - BETTER VERSION"""
    lines = text.split('\n')

    # Find headline position
    headline_idx = -1
    for idx, line in enumerate(lines):
        if len(line) > 20 and (headline[:40] in line or line in headline):
            headline_idx = idx
            break

    if headline_idx == -1:
        headline_idx = 0

    # Start collecting content after headline
    content_start = headline_idx + 1

    # Skip metadata lines (max 5 lines after headline)
    skip_count = 0
    while content_start < len(lines) and skip_count < 5:
        line = lines[content_start].strip()

        # Skip if it looks like metadata
        is_metadata = False

        if len(line) < 15:
            is_metadata = True
        elif re.search(r'TNN|PTI|ANI', line, re.IGNORECASE):
            is_metadata = True
        elif re.search(r'\d{1,2}[:/]\d{1,2}', line):
            is_metadata = True
        elif '|' in line and len(line) < 60:
            is_metadata = True
        elif re.search(r'Printed|Also read', line, re.IGNORECASE):
            is_metadata = True

        if is_metadata:
            content_start += 1
            skip_count += 1
        else:
            break

    # Collect all content lines
    content_lines = []
    for line in lines[content_start:]:
        line = line.strip()

        # Skip very short lines
        if len(line) < 8:
            continue

        # Skip obvious non-content
        if re.match(r'^https?://', line):
            continue

        # Add the line
        content_lines.append(line)

    # Join content
    content = ' '.join(content_lines).strip()

    # Final cleanup
    content = re.sub(r'\s+', ' ', content)
    content = re.sub(r'https?://\S+', '', content)

    # Lower threshold - if we have at least 50 chars, keep it
    if len(content) < 50:
        return "NO_CONTENT"

    return content


# SENTIMENT ANALYSIS - DOMAIN-AWARE SYSTEM


class DomainDetector:
    """Lightweight domain classifier using keyword heuristics"""

    DOMAIN_KEYWORDS = {
        'finance': ['revenue', 'profit', 'stock', 'market', 'investment', 'financial', 'economic',
                    'bank', 'fund', 'trading', 'shares', 'dividend', 'fiscal', 'budget', 'loan'],
        'politics': ['government', 'minister', 'election', 'parliament', 'policy', 'cabinet',
                     'political', 'vote', 'opposition', 'party', 'legislation', 'democracy'],
        'defence': ['DRDO', 'defence', 'defense', 'military', 'army', 'navy', 'air force',
                    'weapon', 'missile', 'security', 'combat', 'forces', 'warfare', 'strategic'],
        'technology': ['AI', 'software', 'tech', 'digital', 'innovation', 'algorithm', 'data',
                       'computing', 'internet', 'cyber', 'startup', 'app', 'platform']
    }

    @staticmethod
    def detect(text):
        """Detect domain based on keyword frequency"""
        text_lower = text.lower()
        scores = {}

        for domain, keywords in DomainDetector.DOMAIN_KEYWORDS.items():
            score = sum(1 for kw in keywords if kw.lower() in text_lower)
            scores[domain] = score

        max_score = max(scores.values())
        if max_score == 0:
            return 'general'

        detected = max(scores, key=scores.get)
        return detected

class SubjectivityFilter:
    """Filter to detect factual vs opinionated content + outcome analysis"""

    FACTUAL_INDICATORS = [
        'announced', 'reported', 'stated', 'according to', 'official', 'data shows',
        'study found', 'research', 'statistics', 'figures', 'documents', 'records'
    ]

    SUBJECTIVE_INDICATORS = [
        'believe', 'feel', 'opinion', 'should', 'must', 'terrible', 'amazing',
        'wonderful', 'awful', 'disappointing', 'exciting', 'shocking'
    ]

    POSITIVE_OUTCOMES = [
        'success', 'achievement', 'breakthrough', 'recovery', 'rebound', 'gain',
        'improvement', 'growth', 'victory', 'win', 'accomplished', 'overcame',
        'resilience', 'triumph', 'progress', 'surge', 'boost', 'rose', 'climbed',
        'selected', 'contract', 'opportunity', 'blessed', 'grateful', 'happy'
    ]

    NEGATIVE_OUTCOMES = [
        'failure', 'loss', 'cut', 'layoff', 'attack', 'killed', 'injured',
        'crisis', 'decline', 'fell', 'dropped', 'breach', 'scandal', 'corruption',
        'arrested', 'terrorism', 'violence', 'shooting', 'dead', 'tragedy'
    ]

    @staticmethod
    def is_factual(text):
        """Check if text is primarily factual"""
        text_lower = text.lower()

        factual_count = sum(1 for ind in SubjectivityFilter.FACTUAL_INDICATORS
                           if ind in text_lower)
        subjective_count = sum(1 for ind in SubjectivityFilter.SUBJECTIVE_INDICATORS
                              if ind in text_lower)

        # If high factual indicators and low subjective, mark as factual
        if factual_count >= 3 and subjective_count <= 1:
            return True

        # If very low emotional language density
        words = text_lower.split()
        if len(words) > 50:
            emotion_ratio = subjective_count / len(words)
            if emotion_ratio < 0.01:  # Less than 1% emotional words
                return True

        return False

    @staticmethod
    def detect_outcome_polarity(text):
        """Detect if article discusses positive or negative outcomes"""
        text_lower = text.lower()

        positive_score = sum(1 for word in SubjectivityFilter.POSITIVE_OUTCOMES
                            if word in text_lower)
        negative_score = sum(1 for word in SubjectivityFilter.NEGATIVE_OUTCOMES
                            if word in text_lower)

        # If clear positive outcome dominates
        if positive_score > negative_score + 2:
            return 'positive_outcome'

        # If clear negative outcome dominates
        if negative_score > positive_score + 2:
            return 'negative_outcome'

        return 'neutral_outcome'

class DomainAwareSentimentAnalyzer:
    """Multi-model sentiment analyzer with domain detection"""

    def __init__(self):
        logger.info("Loading Domain-Aware Sentiment System...")
        self.device = 0 if torch.cuda.is_available() else -1
        device_name = "GPU" if self.device == 0 else "CPU"
        logger.info(f"  -> Using device: {device_name}")

        self.models = {}
        self.domain_detector = DomainDetector()
        self.subjectivity_filter = SubjectivityFilter()

        # Load models
        self._load_models()

        logger.info("✓ Domain-Aware Sentiment System ready")

    def _load_models(self):
        """Load all domain-specific models once"""

        # Finance model
        try:
            logger.info("  -> Loading FinBERT (finance)...")
            self.models['finance'] = hf_pipeline(
                "sentiment-analysis",
                model="yiyanghkust/finbert-tone",
                device=self.device,
                truncation=True,
                max_length=512
            )
            logger.info("     ✓ FinBERT loaded")
        except Exception as e:
            logger.warning(f"     ✗ FinBERT failed: {str(e)[:60]}")
            self.models['finance'] = None

        # Politics model
        try:
            logger.info("  -> Loading RoBERTa (politics)...")
            self.models['politics'] = hf_pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                device=self.device,
                truncation=True,
                max_length=512
            )
            logger.info("     ✓ RoBERTa loaded")
        except Exception as e:
            logger.warning(f"     ✗ RoBERTa failed: {str(e)[:60]}")
            self.models['politics'] = None

        # Defence/Technology/General model
        try:
            logger.info("  -> Loading Siebert (defence/tech/general)...")
            self.models['defence'] = hf_pipeline(
                "sentiment-analysis",
                model="siebert/sentiment-roberta-large-english",
                device=self.device,
                truncation=True,
                max_length=512
            )
            self.models['technology'] = self.models['defence']
            self.models['general'] = self.models['defence']
            logger.info("     ✓ Siebert loaded")
        except Exception as e:
            logger.warning(f"     ✗ Siebert failed: {str(e)[:60]}")
            self.models['defence'] = None
            self.models['technology'] = None
            self.models['general'] = None

    def _normalize_label(self, raw_label):
        """Normalize model-specific labels to standard format"""
        raw_lower = raw_label.lower()

        if 'pos' in raw_lower:
            return 'Positive'
        elif 'neg' in raw_lower:
            return 'Negative'
        else:
            return 'Neutral'

    def analyze_single(self, text):
        """Analyze sentiment for a single text with domain awareness"""

        # Handle invalid input
        if not text or text == "NO_CONTENT" or len(text) < 50:
            return {
                'sentiment_label': 'Unknown',
                'sentiment_score': None,
                'detected_domain': 'unknown',
                'sentiment_model_used': 'none'
            }

        # Step 1: Detect domain
        domain = self.domain_detector.detect(text)

        # Step 2: Check outcome polarity (IMPORTANT for news)
        outcome = self.subjectivity_filter.detect_outcome_polarity(text)

        # Step 3: Override based on clear outcomes
        if outcome == 'positive_outcome':
            # If article clearly discusses positive outcomes (success, recovery, breakthrough)
            # Don't let model get confused by negative words mentioned in past context
            return {
                'sentiment_label': 'Positive',
                'sentiment_score': 0.85,
                'detected_domain': domain,
                'sentiment_model_used': 'outcome_override'
            }
        elif outcome == 'negative_outcome':
            # If article clearly discusses negative outcomes (layoffs, attacks, deaths)
            return {
                'sentiment_label': 'Negative',
                'sentiment_score': 0.85,
                'detected_domain': domain,
                'sentiment_model_used': 'outcome_override'
            }

        # Step 4: Check if purely factual (no strong outcome)
        if self.subjectivity_filter.is_factual(text):
            return {
                'sentiment_label': 'Neutral',
                'sentiment_score': 1.0,
                'detected_domain': domain,
                'sentiment_model_used': 'factual_override'
            }

        # Step 5: Get appropriate model
        model = self.models.get(domain)

        # Fallback if domain model not available
        if model is None:
            for fallback_domain in ['general', 'defence', 'politics', 'finance']:
                if self.models.get(fallback_domain) is not None:
                    model = self.models[fallback_domain]
                    break

        if model is None:
            return {
                'sentiment_label': 'Unknown',
                'sentiment_score': None,
                'detected_domain': domain,
                'sentiment_model_used': 'none'
            }

        # Step 6: Run sentiment analysis
        try:
            result = model(text[:512])[0]

            label = self._normalize_label(result['label'])
            score = round(result['score'], 4)

            # Determine model name
            model_name = 'finbert' if domain == 'finance' else \
                        'roberta' if domain == 'politics' else \
                        'siebert'

            return {
                'sentiment_label': label,
                'sentiment_score': score,
                'detected_domain': domain,
                'sentiment_model_used': model_name
            }

        except Exception as e:
            logger.error(f"Sentiment analysis failed: {str(e)[:80]}")
            return {
                'sentiment_label': 'Unknown',
                'sentiment_score': None,
                'detected_domain': domain,
                'sentiment_model_used': 'error'
            }

    def analyze_batch(self, texts, batch_size=8):
        """Analyze sentiments for batch of texts"""
        results = []
        total = len(texts)

        logger.info(f"Analyzing {total} texts with domain-aware system...")

        for i, text in enumerate(texts):
            result = self.analyze_single(text)
            results.append(result)

            if (i + 1) % 5 == 0 or (i + 1) == total:
                logger.info(f"  -> Processed {i + 1}/{total}")

        return results


# MAIN PIPELINE


def process_pdfs(folder_path):
    """Main processing pipeline"""

    logger.info("="*80)
    logger.info("PDF SENTIMENT ANALYSIS PIPELINE")
    logger.info("="*80)

    # Validate folder
    if not os.path.exists(folder_path):
        logger.error(f"Folder not found: {folder_path}")
        return None

    pdf_files = sorted([f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')])

    if not pdf_files:
        logger.error("No PDFs found")
        return None

    logger.info(f"Found {len(pdf_files)} PDFs\n")

    # Extract metadata
    logger.info("PHASE 1: EXTRACTING METADATA")
    logger.info("-"*80)

    data = []
    texts_for_sentiment = []

    for pdf_file in pdf_files:
        pdf_path = os.path.join(folder_path, pdf_file)

        # Extract text
        raw_text = extract_text_from_pdf(pdf_path)
        if not raw_text:
            logger.warning(f"  -> SKIPPED (no text extracted)")
            continue

        # Light cleaning (keep dates for extraction)
        cleaned_text = clean_text(raw_text)

        # Extract metadata BEFORE aggressive cleaning
        pub_date = extract_date(raw_text)  # Use raw text for date
        headline = extract_headline(cleaned_text)

        # NOW do aggressive cleaning for article content
        content_text = cleaned_text
        # Remove source names
        content_text = re.sub(r'TIMESOFINDIA\.COM|Times of India|\bTOI\b', '', content_text, flags=re.IGNORECASE)
        # Remove bylines
        content_text = re.sub(r'\bTNN\b|\bPTI\b|\bANI\b', '', content_text)
        # Remove dates/times/timezones
        content_text = re.sub(r'[A-Z][a-z]{2,8}\s+\d{1,2},?\s+\d{4}[,\s]+\d{1,2}[:.]\d{2}\s*(?:AM|PM)?\s*(?:IST)?', '', content_text, flags=re.IGNORECASE)
        content_text = re.sub(r'\b(?:IST|EST|PST|GMT)\b', '', content_text)
        # Remove city prefixes
        content_text = re.sub(r'^[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*:\s*', '', content_text, flags=re.MULTILINE)

        article_content = extract_article_content(content_text, headline)

        data.append({
            'file_name': pdf_file,
            'publication_date': pub_date,
            'headline': headline,
            'article_content': article_content,
            'sentiment_label': None,
            'sentiment_score': None
        })

        texts_for_sentiment.append(article_content)

        logger.info(f"  -> {pdf_file}")
        logger.info(f"     Headline: {headline[:70]}...")
        logger.info(f"     Date: {pub_date}")
        logger.info(f"     Content: {len(article_content)} chars\n")

    if not data:
        logger.error("No data extracted")
        return None

    df = pd.DataFrame(data)

    # Sentiment analysis
    logger.info("\nPHASE 2: SENTIMENT ANALYSIS (DOMAIN-AWARE)")
    logger.info("-"*80)

    analyzer = DomainAwareSentimentAnalyzer()
    results = analyzer.analyze_batch(texts_for_sentiment)

    df['sentiment_label'] = [r['sentiment_label'] for r in results]
    df['sentiment_score'] = [r['sentiment_score'] for r in results]
    df['detected_domain'] = [r['detected_domain'] for r in results]
    df['sentiment_model_used'] = [r['sentiment_model_used'] for r in results]

    # Export
    logger.info("\nPHASE 3: EXPORTING TO EXCEL")
    logger.info("-"*80)

    output_path = os.path.join(folder_path, 'pdf_news_sentiment_dataset.xlsx')

    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='News_Sentiment', index=False)

        worksheet = writer.sheets['News_Sentiment']

        # Set column widths
        worksheet.column_dimensions['A'].width = 20
        worksheet.column_dimensions['B'].width = 15
        worksheet.column_dimensions['C'].width = 50
        worksheet.column_dimensions['D'].width = 60
        worksheet.column_dimensions['E'].width = 15
        worksheet.column_dimensions['F'].width = 15
        worksheet.column_dimensions['G'].width = 15
        worksheet.column_dimensions['H'].width = 20

    logger.info(f"Saved: {output_path}")

    # Display results
    logger.info("\n" + "="*80)
    logger.info("FINAL DATASET")
    logger.info("="*80)
    print(df.to_string(index=False))

    logger.info("\n" + "="*80)
    logger.info("PIPELINE COMPLETED SUCCESSFULLY!")
    logger.info("="*80)

    return df


# RUN IN COLAB


if __name__ == "__main__":
    # Mount Drive
    try:
        from google.colab import drive
        drive.mount('/content/drive', force_remount=True)
        logger.info("Google Drive mounted\n")
    except:
        logger.warning("Not in Colab (local mode)")

    # SET YOUR FOLDER PATH HERE
    FOLDER_PATH = '/content/drive/My Drive/hello'

    # Run pipeline
    df_result = process_pdfs(FOLDER_PATH)

    # Download
    if df_result is not None:
        try:
            from google.colab import files
            output_file = os.path.join(FOLDER_PATH, 'pdf_news_sentiment_dataset.xlsx')
            if os.path.exists(output_file):
                files.download(output_file)
                logger.info("\nFile ready for download!")
        except:
            logger.info("\nFile saved in your Drive folder!")